- New loss function (--> Until lunch) 
- Implpement LSTM 
- Implement WaveNet
- Set-up similar HyperParamTuning script for these
- Hypertuning over-night 
- Train the different models on (5ish) different datasets. Â¨
- How to make transformer autoreggressive. 
- Try models with mu-law data.

If time: 
	- Log scale
	- More consideration of loss function
	- Hypertuning for each dataset... (i.e. not same config for all the datasets). 


 - Check that time and freq are interpreted correctly by model... 
 - Old trainRAMT uses fs and nperseg only for stft
 - Also warm-up steps of 4000 for only three samples per epoch is v large? What could this indicat? 



x- Overfit with abs(real()) to see if timber goes 
x- Overfit with TrainRAMT2 script
x- Run for only 100 epochs to see if still sensible results (want to know roughly error)... 
	--> Constant values... 
x- Plot spectral plots when saving wav files... 
x- Look at loss-function for only positive values to arrive at appropriate threshold.
x- Re-do preprocessing with .15 seconds to reduce sequence length, try model with bs = 16
x- Implement so that for every hypertuning setup we also save two wav files and two pred plots
 - Try longformer with OLD Data that was too big for transformer
x- Train with full data to see if still just noise
	- If yes (in this order): 
		- Try my loss function
		- Remove duplicate labels... 
	- If no: 
		- Still try and compare with my loss function
x- Try the different scalers to check which is best.. 
x- Check with Longformer and set-up saving/inference stuff for longformer as well. 
 - Hypertuning script 
 - Long runs over the weekend
